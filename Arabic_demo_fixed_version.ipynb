{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486dcf2d-c566-4ce8-9c5c-32954afe1192",
   "metadata": {},
   "source": [
    "# 1- Run the whisper medium model to do the transcribing, from English audio to text. Using well-defined data, to show the computing sources requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb581172-9ab0-4029-b9eb-206597e2a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating validation split: 100%|██████████| 1/1 [00:00<00:00, 112.68 examples/s]\n",
      "/home/lm2445/.conda/envs/arabic_proj/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of up guards and atom paintings, and Mason's exquisite idles are as national as a jingo poem. Mr. Birkett Foster's landscapes smile at one much in the same way that Mr. Carcar used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap in the back before he says, like a champ pooler in a Turkish bath. Next man.\n",
      "Peak GPU memory usage: 1625.11 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "#model_id = \"openai/whisper-medium\"\n",
    "#model_id = \"/home/lm2445/Arabic_models/models--openai--whisper-medium/snapshots/abdf7c39ab9d0397620ccaea8974cc764cd0953e\"  \n",
    "model_id = \"lm2445/for_transribing\"\n",
    "\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True   # <-- Add this line, all the input longer than 30 sec\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "peak_memory = torch.cuda.max_memory_allocated()\n",
    "print(f\"Peak GPU memory usage: {peak_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355fb6a-cb52-47bc-bf81-d05432cdfa9f",
   "metadata": {},
   "source": [
    "# 2- using local Arabic audio file as input - show how the model work for Arabic audio -> Arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ab819a-cef6-4b3d-81ce-983a564bbf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " مونا ازايك يا مونا؟ مونا زاكي يا حبيبتي زاكي يا مونا اشكرك جدا يا مونا شكري يا حبيبتي انا فخور بيك جدا يا مونا\n",
      "Peak GPU memory usage: 3082.37 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "file_path = \"/home/lm2445/project_pi_sjf37/lm2445/Arabic/V8.wav\"\n",
    "waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0)\n",
    "\n",
    "sample = {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": sr}\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"lm2445/for_transribing\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# FIXED: Only remove forced tokens from CONFIG, not tokenizer\n",
    "# ------------------------------------------------------\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = None\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Pipeline\n",
    "# ------------------------------------------------------\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=device,\n",
    "    torch_dtype=torch_dtype,\n",
    "    return_timestamps=False,\n",
    "    generate_kwargs={\n",
    "        \"language\": \"ar\",\n",
    "        \"task\": \"transcribe\",\n",
    "        \"forced_decoder_ids\": None,\n",
    "        \"suppress_tokens\": None,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Only keep 30 seconds for testing\n",
    "sample[\"array\"] = sample[\"array\"][: int(30 * sample[\"sampling_rate\"])]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "\n",
    "peak_memory = torch.cuda.max_memory_allocated()\n",
    "print(f\"Peak GPU memory usage: {peak_memory/1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df036eb2-5b61-4ffd-83c3-618ae210a1bd",
   "metadata": {},
   "source": [
    "# 3-Try deidentify via Camelbert\n",
    "## the Camelbert can detect the location of the important words and show their locations as ouput\n",
    "## PER: person; ORG: organization; LOC: location; MISC: Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b471d-8582-4aae-840c-8953406ac63a",
   "metadata": {},
   "source": [
    "## 3.1 two examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469043fc-2f47-420a-98c3-6153a5c23ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lm2445/for_deidentify were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': np.float32(0.9928443), 'index': 2, 'word': 'أبوظبي', 'start': 6, 'end': 12}, {'entity': 'B-LOC', 'score': np.float32(0.95888805), 'index': 8, 'word': 'الإمارات', 'start': 33, 'end': 41}, {'entity': 'I-LOC', 'score': np.float32(0.9397786), 'index': 9, 'word': 'العربية', 'start': 42, 'end': 49}, {'entity': 'I-LOC', 'score': np.float32(0.96159947), 'index': 10, 'word': 'المتحدة', 'start': 50, 'end': 57}]\n",
      "Peak GPU memory usage: 1883.08 MB\n"
     ]
    }
   ],
   "source": [
    "# try deidentify \n",
    "# https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#local_model = \"/home/lm2445/Arabic_models/models--CAMeL-Lab--bert-base-arabic-camelbert-mix-ner/snapshots/40b8059a6f3bfbb49d64038e131f49b93cc37417\"\n",
    "ner = pipeline('ner', model='lm2445/for_deidentify')\n",
    "# ner = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=local_model,\n",
    "#     tokenizer=local_model,\n",
    "#     device=0 if torch.cuda.is_available() else -1,\n",
    "# )\n",
    "results = ner(\"إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع\")\n",
    "print (results)\n",
    "peak_memory = torch.cuda.max_memory_allocated()\n",
    "print(f\"Peak GPU memory usage: {peak_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae280c06-0b15-45b4-9d2e-e92556d0d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lm2445/for_deidentify were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PERS', 'score': np.float32(0.54886025), 'index': 9, 'word': 'مونا', 'start': 31, 'end': 35}, {'entity': 'B-PERS', 'score': np.float32(0.71680826), 'index': 13, 'word': 'مونا', 'start': 45, 'end': 49}, {'entity': 'B-PERS', 'score': np.float32(0.8773864), 'index': 14, 'word': 'مونا', 'start': 50, 'end': 54}, {'entity': 'I-PERS', 'score': np.float32(0.85757035), 'index': 15, 'word': 'زا', 'start': 55, 'end': 57}, {'entity': 'I-PERS', 'score': np.float32(0.8754845), 'index': 16, 'word': '##كي', 'start': 57, 'end': 59}, {'entity': 'B-PERS', 'score': np.float32(0.91140044), 'index': 24, 'word': 'زا', 'start': 88, 'end': 90}, {'entity': 'B-PERS', 'score': np.float32(0.56098473), 'index': 25, 'word': '##كي', 'start': 90, 'end': 92}, {'entity': 'B-PERS', 'score': np.float32(0.64975023), 'index': 27, 'word': 'مونا', 'start': 96, 'end': 100}, {'entity': 'B-PERS', 'score': np.float32(0.64129), 'index': 38, 'word': 'مونا', 'start': 141, 'end': 145}, {'entity': 'B-PERS', 'score': np.float32(0.59759027), 'index': 61, 'word': 'مونا', 'start': 228, 'end': 232}]\n",
      "Peak GPU memory usage: 2296.03 MB\n"
     ]
    }
   ],
   "source": [
    "# try deidentify \n",
    "# https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#local_model = \"/home/lm2445/Arabic_models/models--CAMeL-Lab--bert-base-arabic-camelbert-mix-ner/snapshots/40b8059a6f3bfbb49d64038e131f49b93cc37417\"\n",
    "ner = pipeline('ner', model='lm2445/for_deidentify')\n",
    "# ner = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=local_model,\n",
    "#     tokenizer=local_model,\n",
    "#     device=0 if torch.cuda.is_available() else -1,\n",
    "# )\n",
    "results = ner(\" اه قل الو انت اعترفين الو الو مونا ازايك يا مونا مونا زاكي كل سنة انتو طيبين يا حبيبتي زاكي يا مونا احبك جدا وكل سنة حبيبك طيب اشكرك جدا يا مونا و الف الف مبروك ع الموثلسل الف الف الف مبروك شكري حبيبتي انا انا فخور بيكي جدا يا مونا فخور حبيبتك بالتحديد لما تبقى تبقى فقربية بحاجة شرف\")\n",
    "print (results)\n",
    "peak_memory = torch.cuda.max_memory_allocated()\n",
    "print(f\"Peak GPU memory usage: {peak_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c61ee-3fff-48af-8d2d-558085e8c2b6",
   "metadata": {},
   "source": [
    "## 3.2 Whole Deidentify Process: text-> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca7cfbd-83cc-4c6b-a4a9-4554b29a0552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lm2445/for_deidentify were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "/home/lm2445/.conda/envs/arabic_proj/lib/python3.13/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Text ===\n",
      "سافرت الدكتورة ليلى حمدان مع زميلها الدكتور عمر الكيلاني إلى مدينة جنيف لزيارة مقر منظمة الصحة العالمية، حيث عُقِد اجتماع رسمي مع ممثلين من منظمة الأمم المتحدة لإطلاق مشروع بحثي جديد حول الذكاء الاصطناعي في الرعاية الصحية. وبعد الاجتماع، توجّه الوفد إلى جامعة أكسفورد في لندن لمناقشة تعاون علمي مع معهد البيانات الطبية. وأكدت ليلى حمدان أن نتائج الدراسة ستُنشر بالتعاون مع جامعة القاهرة ومركز الابتكار الرقمي، بينما أشار عمر الكيلاني إلى أهمية تطوير أدوات تعتمد على تحليل اللغة الطبيعية والتعلم العميق لتطبيقها في أنظمة المستشفيات في الشرق الأوسط.\n",
      "\n",
      "=== NER Output ===\n",
      "[{'entity_group': 'PERS', 'score': np.float32(0.96569043), 'word': 'ليلى حمدان', 'start': 15, 'end': 25}, {'entity_group': 'PERS', 'score': np.float32(0.9855659), 'word': 'عمر الكيلاني', 'start': 44, 'end': 56}, {'entity_group': 'LOC', 'score': np.float32(0.99765825), 'word': 'جنيف', 'start': 67, 'end': 71}, {'entity_group': 'ORG', 'score': np.float32(0.99653834), 'word': 'منظمة الصحة العالمية', 'start': 83, 'end': 103}, {'entity_group': 'ORG', 'score': np.float32(0.99699754), 'word': 'منظمة الأمم المتحدة', 'start': 140, 'end': 159}, {'entity_group': 'ORG', 'score': np.float32(0.983192), 'word': 'جامعة أكسفورد', 'start': 254, 'end': 267}, {'entity_group': 'LOC', 'score': np.float32(0.99766636), 'word': 'لندن', 'start': 271, 'end': 275}, {'entity_group': 'ORG', 'score': np.float32(0.91919595), 'word': 'معهد البيانات الطبية', 'start': 298, 'end': 318}, {'entity_group': 'PERS', 'score': np.float32(0.944028), 'word': 'ليلى حمدان', 'start': 326, 'end': 336}, {'entity_group': 'ORG', 'score': np.float32(0.9739082), 'word': 'جامعة القاهرة', 'start': 373, 'end': 386}, {'entity_group': 'PERS', 'score': np.float32(0.9685958), 'word': 'عمر الكيلاني', 'start': 421, 'end': 433}, {'entity_group': 'LOC', 'score': np.float32(0.9715128), 'word': 'الشرق الأوسط', 'start': 534, 'end': 546}]\n",
      "\n",
      "=== De-identified Text ===\n",
      "سافرت الدكتورة <PERS> مع زميلها الدكتور <PERS> إلى مدينة <LOC> لزيارة مقر <ORG>، حيث عُقِد اجتماع رسمي مع ممثلين من <ORG> لإطلاق مشروع بحثي جديد حول الذكاء الاصطناعي في الرعاية الصحية. وبعد الاجتماع، توجّه الوفد إلى <ORG> في <LOC> لمناقشة تعاون علمي مع <ORG>. وأكدت <PERS> أن نتائج الدراسة ستُنشر بالتعاون مع <ORG> ومركز الابتكار الرقمي، بينما أشار <PERS> إلى أهمية تطوير أدوات تعتمد على تحليل اللغة الطبيعية والتعلم العميق لتطبيقها في أنظمة المستشفيات في <LOC>.\n",
      "\n",
      "Peak GPU memory usage: 2296.03 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# -----------------------\n",
    "# Load NER model\n",
    "# -----------------------\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "#ner = pipeline(\"ner\", model=\"CAMeL-Lab/bert-base-arabic-camelbert-mix-ner\", grouped_entities=True)\n",
    "#local_model = \"/home/lm2445/Arabic_models/models--CAMeL-Lab--bert-base-arabic-camelbert-mix-ner/snapshots/40b8059a6f3bfbb49d64038e131f49b93cc37417\"\n",
    "#ner = pipeline('ner', model='lm2445/for_deidentify')\n",
    "ner = pipeline(\"ner\",\n",
    "               model=\"lm2445/for_deidentify\",\n",
    "               grouped_entities=True)\n",
    "\n",
    "# ner = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=local_model,\n",
    "#     tokenizer=local_model,\n",
    "#     device=0 if torch.cuda.is_available() else -1,\n",
    "#     grouped_entities=True,\n",
    "# )\n",
    "\n",
    "# -----------------------\n",
    "# Input sentence\n",
    "# -----------------------\n",
    "#text = \"اه قل الو انت اعترفين الو الو مونا ازايك يا مونا مونا زاكي كل سنة انتو طيبين يا حبيبتي زاكي يا مونا احبك جدا وكل سنة حبيبك طيب اشكرك جدا يا مونا و الف الف مبروك ع الموثلسل الف الف الف مبروك شكري حبيبتي انا انا فخور بيكي جدا يا مونا فخور حبيبتك بالتحديد لما تبقى تبقى فقربية بحاجة شرف\"\n",
    "text = \"سافرت الدكتورة ليلى حمدان مع زميلها الدكتور عمر الكيلاني إلى مدينة جنيف لزيارة مقر منظمة الصحة العالمية، حيث عُقِد اجتماع رسمي مع ممثلين من منظمة الأمم المتحدة لإطلاق مشروع بحثي جديد حول الذكاء الاصطناعي في الرعاية الصحية. وبعد الاجتماع، توجّه الوفد إلى جامعة أكسفورد في لندن لمناقشة تعاون علمي مع معهد البيانات الطبية. وأكدت ليلى حمدان أن نتائج الدراسة ستُنشر بالتعاون مع جامعة القاهرة ومركز الابتكار الرقمي، بينما أشار عمر الكيلاني إلى أهمية تطوير أدوات تعتمد على تحليل اللغة الطبيعية والتعلم العميق لتطبيقها في أنظمة المستشفيات في الشرق الأوسط.\"\n",
    "# -----------------------\n",
    "# Run NER\n",
    "# -----------------------\n",
    "results = ner(text)\n",
    "\n",
    "# -----------------------\n",
    "# Build replacements\n",
    "# -----------------------\n",
    "deidentified_text = text\n",
    "\n",
    "for entity in results:\n",
    "    ent_text = entity['word']\n",
    "    ent_label = entity['entity_group']\n",
    "\n",
    "    # CAMeL labels: PER, ORG, LOC, MISC\n",
    "    placeholder = f\"<{ent_label}>\"\n",
    "\n",
    "    # Replace entity text in the sentence (use regex to avoid partial overlaps)\n",
    "    pattern = re.escape(ent_text)\n",
    "    deidentified_text = re.sub(pattern, placeholder, deidentified_text)\n",
    "\n",
    "# -----------------------\n",
    "# Print results\n",
    "# -----------------------\n",
    "print(\"=== Original Text ===\")\n",
    "print(text)\n",
    "print(\"\\n=== NER Output ===\")\n",
    "print(results)\n",
    "print(\"\\n=== De-identified Text ===\")\n",
    "print(deidentified_text)\n",
    "\n",
    "# -----------------------\n",
    "# GPU Memory\n",
    "# -----------------------\n",
    "peak = torch.cuda.max_memory_allocated()\n",
    "print(f\"\\nPeak GPU memory usage: {peak / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e6abc-7a65-47c4-b042-ae94dbfa00d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arabic_proj_2025",
   "language": "python",
   "name": "arabic_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
